from typing import ClassVar, Dict, List
from sklearn.pipeline import Pipeline
from transformers import pipeline, logging
from scipy.special import softmax
import numpy as np
import csv

'''
Let M be a masked language model with vocab-
ulary V and mask token ∈ V , and let L be a
set of labels for our target classification task A.
We write an input for task A as a sequence of
phrases x = (s1 , . . . , sk ) with si ∈ V* ; for ex-
ample, k = 2 if A is textual inference (two input
sentences). We define a pattern to be a function P
that takes x as input and outputs a phrase or sen-
tence P (x) ∈ V* that contains exactly one mask
token, i.e., its output can be viewed as a cloze ques-
tion. Furthermore, we define a verbalizer as an
injective function v : L → V that maps each label
to a word from M's vocabulary. We refer to (P, v)
as a pattern-verbalizer pair (PVP).
'''

logging.set_verbosity_error()
unmasker = pipeline('fill-mask', model='roberta-base')
def unmask(s) -> List[str]:
	ret_val = unmasker(s)
	vals = []
	for r in ret_val:
		vals.append(r['token_str'])
	return vals

class Pattern:
	# We write an input for task A as a sequence of 
	# phrases x = (s1 , . . . , sk ) with si ∈ V*
	phrases: ClassVar[str]

	def __init__(self, phrase: str):
		self.phrase = phrase

	# We define a pattern to be a function Pthat takes x as input and outputs a 
	# phrase or sentence P (x) ∈ V * that contains exactly one mask token
	def resolve(self, input: str) -> str:
		return self.phrase.replace('<a>', input)

class Verbalizer:
	labels: ClassVar[List[str]]

	def __init__(self):
		# [{ sentiment: String, values: [String] }]
		# let L be a set of labels for our target classification task A
		self.labels = []

	def add_value(self, value: str) -> None:
		if value in self.labels:
			return
		self.labels.append(value)

	# we define a verbalizer as an injective function v : L → V that 
	# maps each label to a word from M's vocabulary.
	def get_label(self, value: str) -> int:
		i = 0
		for l in self.labels:
			if l == value:
				return i
			i += 1
		return -1

'''
Using a PVP (P, v) enables us to solve task A as
follows: Given an input x, we apply P to obtain an
input representation P (x), which is then processed
by M to determine the label y ∈ L for which
v(y) is the most likely substitute for the mask.
'''

# We refer to (P, v) as a pattern-verbalizer pair (PVP).
class PVP:
	runs: ClassVar[int] = 0
	model: ClassVar[Pipeline]
	verbalizer: ClassVar[Verbalizer]
	totals: ClassVar[Dict[str, int]]

	def __init__(self, model: Pipeline, verbalizer: Verbalizer):
		# Let M be a masked language model with vocabulary V and mask token ∈ V
		self.model = model
		self.verbalizer = verbalizer
		self.totals = { }

	# Given an input x
	def run(self, pattern: Pattern, inp: str) -> str:
		# we apply P to obtain an input representation P (x)
		p: str = pattern.resolve(inp)
		# which is then processed by M to determine the label y ∈ L
		model_ret: List[str] = unmask(p)
		l = model_ret[0]
		# for which v(y) is the most likely substitute for the mask
		self.verbalizer.add_value(l)
		if l in self.totals:
			self.totals[l] += 1
		else:
			self.totals[l] = 1
		self.runs += 1
		return l
	
	def calculate_distribution(self):
		probabilities = []
		for l in self.totals.keys():
			total = self.totals[l]
			prob = total / self.runs
			probabilities.append(prob)
		sm = softmax(np.array(probabilities))
		sm_labels = [ ]
		i = 0
		for l in self.totals.keys():
			sm_labels.append((l, sm[i]))
			i += 1
		return sm_labels


v = Verbalizer()
pvp = PVP(unmasker, v)

patterns = [
	'It was <mask>. <a>',
	'<a>. All in all, it was <mask>',
	'Just <mask>! <a>',
	'<a>. In summary, the restaurant is <mask>'
]

with open('data/yelp.csv') as csvfile:
	rdr = csv.reader(csvfile)
	i = 0
	for row in rdr:
		pattern = Pattern(patterns[0])
		p = pattern.resolve(row[4])
		if len(p) > 3000:
			continue
		v = pvp.run(pattern, row[4])
		if i > 50:
			break
		i += 1

	sm = pvp.calculate_distribution()
	for l in sm:
		print(l)

	# TODO: calculate loss from actual data depending upon 
	# pattern and iterate over patterns to determine differences between them

	# We can apply this principle to generative functions by using a prompt as a pattern
	# determining a label from the output