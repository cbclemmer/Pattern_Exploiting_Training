import time
from typing import ClassVar, Dict, List, Tuple
from sklearn.pipeline import Pipeline
from transformers import pipeline, logging
from scipy.special import softmax
import numpy as np
import csv

'''
Let M be a masked language model with vocab-
ulary V and mask token ∈ V , and let L be a
set of labels for our target classification task A.
We write an input for task A as a sequence of
phrases x = (s1 , . . . , sk ) with si ∈ V* ; for ex-
ample, k = 2 if A is textual inference (two input
sentences). We define a pattern to be a function P
that takes x as input and outputs a phrase or sen-
tence P (x) ∈ V* that contains exactly one mask
token, i.e., its output can be viewed as a cloze ques-
tion. Furthermore, we define a verbalizer as an
injective function v : L → V that maps each label
to a word from M's vocabulary. We refer to (P, v)
as a pattern-verbalizer pair (PVP).
'''

logging.set_verbosity_error()
def unmask(m: Pipeline, s: str) -> List[str]:
	ret_val = m(s)
	vals = []
	for r in ret_val:
		vals.append(r['token_str'])
	return vals

# We define a pattern to be a function Pthat takes x as input and outputs a 
# phrase or sentence P (x) ∈ V * that contains exactly one mask token
class Pattern:
	# We write an input for task A as a sequence of 
	# phrases x = (s1 , . . . , sk ) with si ∈ V*
	phrases: ClassVar[str]

	def __init__(self, phrase: str):
		self.phrase = phrase

	# we apply P to obtain an input representation P(x)
	def get_representation(self, x: str) -> str:
		return self.phrase.replace('<a>', x)


# The mapping is dependant upon the pattern and the model
class Verbalizer:
	label_mapping: ClassVar[Dict[str, List[str]]]

	def __init__(self):
		# [{ sentiment: String, values: [String] }]
		# let L be a set of labels for our target classification task A
		self.label_mapping = { }

	def add_value(self, label: str, value: str) -> None:
		if not label in self.label_mapping:
			self.label_mapping[label] = [ ]
		if value in self.label_mapping[label]:
			return
		self.label_mapping[label].append(value)

	# we define a verbalizer as an injective function v : L → V that 
	# maps each label to a word from M's vocabulary.
	def get_label(self, value: str) -> str | None:
		for label, values in self.label_mapping.items():
			if value in values:
				return label
		return None
	
	def get_values(self, search_label: str) -> List[str] | None:
		for label, values in self.label_mapping.items():
			if label == search_label:
				return values
		return None
	
	def get_one_hot(self, label: str) -> List[int]:
		one_hot = [ ]
		for l in self.label_mapping.keys():
			if l == label:
				one_hot.append(1)
			else:
				one_hot.append(0)
		return one_hot

'''
Using a PVP (P, v) enables us to solve task A as
follows: Given an input x, we apply P to obtain an
input representation P (x), which is then processed
by M to determine the label y ∈ L for which
v(y) is the most likely substitute for the mask.
'''

# We refer to (P, v) as a pattern-verbalizer pair (PVP).
# depends on a model
class PatternVerbalizerPair:
	runs: ClassVar[int] = 0
	model: ClassVar[Pipeline]
	pattern: ClassVar[Pattern]
	verbalizer: ClassVar[Verbalizer]
	totals: ClassVar[Dict[str, int]]
	max_len: ClassVar[int]
	entropy: ClassVar[float | None]

	def __init__(self, model: Pipeline, pattern_str: str, max_len = 2000):
		# Let M be a masked language model with vocabulary V and mask token ∈ V
		self.model = model
		self.max_len = max_len
		self.pattern = Pattern(pattern_str)
		self.verbalizer = Verbalizer()
		self.entropy = None
		self.totals = { }
		self.probabilities = { }

	# Given an input x
	# y is label
	# vy is word in V
	# z is the training text
	def train(self, z: str, y: str) -> None:
		px: str = self.pattern.get_representation(z)
		if (len(px) > self.max_len):
			return
		# which is then processed by M to determine the label y ∈ L
		vy: str = unmask(self.model, px)[0]
		# for which v(y) is the most likely substitute for the mask
		self.verbalizer.add_value(y, vy)

		if not vy in self.totals:
			self.totals[vy] = { }
		if not y in self.totals[vy]:
			self.totals[vy][y] = 0

		self.totals[vy][y] += 1
		self.runs += 1
	
	def test(self, z: str) -> str:
		px = self.pattern.get_representation(z)
		if (len(px) > self.max_len):
			return (None, None)
		vy = unmask(self.model, px)[0]
		return (vy, self.verbalizer.get_label(vy))

	def calculate_probabilities(self):
		if len(self.totals.keys()) == 0:
			return
		for v, labels in self.totals.items():
			total = 0
			for label_total in labels.values():
				total += label_total
			probabilites = [ ]
			for l in self.verbalizer.label_mapping.keys():
				if not l in labels:
					probabilites.append(0)
				else:
					probabilites.append(labels[l])
			self.probabilities[v] = softmax(np.array(probabilites))
	
	def train_set(self, set: List[Tuple[str, str]]):
		for (text, truth) in set:
			self.train(text, truth)
		self.calculate_probabilities()

	def calculate_entropy(self, set: List[str]):
		total_entropy = 0
		for text in set:
			(v, label) = self.test(text)
			if v == None:
				continue
			one_hot = self.verbalizer.get_one_hot(label)
			prediction = self.probabilities[v]
			total_entropy += -np.sum(one_hot * np.log(prediction), axis=-1)
		self.entropy = total_entropy
		return total_entropy


unmasker = pipeline('fill-mask', model='roberta-base', device='cuda:0')

patterns = [
	'It was <mask>. <a>',
	'<a>. All in all, it was <mask>',
	'Just <mask>! <a>',
	'<a>. In summary, the restaurant is <mask>'
]

rating_index = 3
text_index = 4

total_time = time.time()
pvps = []
for p in patterns:
	pvp = PatternVerbalizerPair(unmasker, p)
	with open('data/yelp.csv') as csvfile:
		rdr = csv.reader(csvfile)
		i = 0
		train_set = []
		for row in rdr:
			train_set.append((row[text_index], row[rating_index]))
			if i > 500:
				break
			i += 1
		print('\n\n' + p)
		train_time = time.time()
		pvp.train_set(train_set)
		print(f"Training took {(time.time() - train_time):.2f}s")
		test_set = map(lambda x: x[0], train_set)
		test_time = time.time()
		entropy = pvp.calculate_entropy(test_set)
		print(f"Testing took {(time.time() - test_time):.2f}s")
		pvps.append((p, entropy))
print('\n\n\nResults:')
for (p, entropy) in pvps:
	print(f"{p}: {entropy:.2f}")
print(f"Total execution time: {time.time() - total_time:.2f}")

# with open('data/yelp.csv') as csvfile:
# 	rdr = csv.reader(csvfile)
# 	i = 0
# 	for row in rdr:
# 		pattern = Pattern(patterns[0])
# 		p = pattern.get_representation(row[4])
# 		if len(p) > 3000:
# 			continue
# 		v = pvp.run(pattern, row[4])
# 		if i > 50:
# 			break
# 		i += 1

# 	sm = pvp.calculate_distribution()
# 	for l in sm:
# 		print(l)
# 	for k in pvp.verbalizer.label_mapping.keys

	# TODO: calculate loss from actual data depending upon 
	# pattern and iterate over patterns to determine differences between them

	# We can apply this principle to generative functions by using a prompt as a pattern
	# determining a label from the output