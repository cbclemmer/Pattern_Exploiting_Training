from typing import ClassVar, Dict, List
from sklearn.pipeline import Pipeline
from transformers import pipeline, logging
from scipy.special import softmax
import numpy as np
import csv

'''
Let M be a masked language model with vocab-
ulary V and mask token ∈ V , and let L be a
set of labels for our target classification task A.
We write an input for task A as a sequence of
phrases x = (s1 , . . . , sk ) with si ∈ V* ; for ex-
ample, k = 2 if A is textual inference (two input
sentences). We define a pattern to be a function P
that takes x as input and outputs a phrase or sen-
tence P (x) ∈ V* that contains exactly one mask
token, i.e., its output can be viewed as a cloze ques-
tion. Furthermore, we define a verbalizer as an
injective function v : L → V that maps each label
to a word from M's vocabulary. We refer to (P, v)
as a pattern-verbalizer pair (PVP).
'''

logging.set_verbosity_error()
def unmask(m: Pipeline, s: str) -> List[str]:
	ret_val = m(s)
	vals = []
	for r in ret_val:
		vals.append(r['token_str'])
	return vals

# We define a pattern to be a function Pthat takes x as input and outputs a 
# phrase or sentence P (x) ∈ V * that contains exactly one mask token
class Pattern:
	# We write an input for task A as a sequence of 
	# phrases x = (s1 , . . . , sk ) with si ∈ V*
	phrases: ClassVar[str]

	def __init__(self, phrase: str):
		self.phrase = phrase

	# we apply P to obtain an input representation P(x)
	def get_representation(self, x: str) -> str:
		return self.phrase.replace('<a>', x)


# The mapping is dependant upon the pattern and the model
class Verbalizer:
	label_mapping: ClassVar[Dict[str, List[str]]]

	def __init__(self):
		# [{ sentiment: String, values: [String] }]
		# let L be a set of labels for our target classification task A
		self.label_mapping = { }

	def add_value(self, label: str, value: str) -> None:
		if not label in self.label_mapping:
			self.label_mapping[label] = [ ]
		if value in self.label_mapping[label]:
			return
		self.label_mapping[label].append(value)

	# we define a verbalizer as an injective function v : L → V that 
	# maps each label to a word from M's vocabulary.
	def get_label(self, value: str) -> str | None:
		for label, values in self.label_mapping.items():
			if value in values:
				return label
		return None
	
	def get_values(self, search_label: str) -> List[str] | None:
		for label, values in self.label_mapping.items():
			if label == search_label:
				return values
		return None

'''
Using a PVP (P, v) enables us to solve task A as
follows: Given an input x, we apply P to obtain an
input representation P (x), which is then processed
by M to determine the label y ∈ L for which
v(y) is the most likely substitute for the mask.
'''

# We refer to (P, v) as a pattern-verbalizer pair (PVP).
# depends on a model
class PatternVerbalizerPair:
	runs: ClassVar[int] = 0
	model: ClassVar[Pipeline]
	pattern: ClassVar[Pattern]
	verbalizer: ClassVar[Verbalizer]
	totals: ClassVar[Dict[str, int]]

	def __init__(self, model: Pipeline, pattern: Pattern, verbalizer: Verbalizer):
		# Let M be a masked language model with vocabulary V and mask token ∈ V
		self.model = model
		self.pattern = pattern
		self.verbalizer = verbalizer
		self.totals = { }

	# Given an input x
	# y is label
	# vy is word in V
	# z is the training text
	def train(self, z: str, y: str) -> None:
		px: str = self.pattern.get_representation(z)
		# which is then processed by M to determine the label y ∈ L
		vy: str = unmask(self.model, px)[0]
		# for which v(y) is the most likely substitute for the mask
		self.verbalizer.add_value(y, vy)

		if not y in self.totals:
			self.totals[y] = { }
		if not vy in self.totals[y]:
			self.totals[y][vy] = 0

		self.totals[y][vy] += 1
		self.runs += 1
	
	def test(self, z: str) -> str:
		px: self.pattern.get_representation(z)
		vy = unmask(self.model, px)[0]
		return self.verbalizer.get_label(vy)

	# def calculate_distribution(self):
	# 	probabilities = []
	# 	for l in self.totals.keys():
	# 		total = self.totals[l]
	# 		prob = total / self.runs
	# 		probabilities.append(prob)
	# 	sm = softmax(np.array(probabilities))
	# 	sm_labels = [ ]
	# 	i = 0
	# 	for l in self.totals.keys():
	# 		sm_labels.append((l, sm[i]))
	# 		i += 1
	# 	return sm_labels


v = Verbalizer()
unmasker = pipeline('fill-mask', model='roberta-base')

patterns = [
	'It was <mask>. <a>',
	'<a>. All in all, it was <mask>',
	'Just <mask>! <a>',
	'<a>. In summary, the restaurant is <mask>'
]

rating_index = 3
text_index = 4

pvps = []
for p in patterns:
	pattern = Pattern(p)
	pvp = PatternVerbalizerPair(unmasker, pattern, v)
	with open('data/yelp.csv') as csvfile:
		rdr = csv.reader(csvfile)
		i = 0
		for row in rdr:
			rating = row[rating_index]
			text = row[text_index]
			p = pvp.pattern.get_representation(text)
			if len(p) > 3000:
				continue
			v = pvp.train(text, rating)
			if i > 50:
				break
			i += 1
	pvps.append(pvp)


# with open('data/yelp.csv') as csvfile:
# 	rdr = csv.reader(csvfile)
# 	i = 0
# 	for row in rdr:
# 		pattern = Pattern(patterns[0])
# 		p = pattern.get_representation(row[4])
# 		if len(p) > 3000:
# 			continue
# 		v = pvp.run(pattern, row[4])
# 		if i > 50:
# 			break
# 		i += 1

# 	sm = pvp.calculate_distribution()
# 	for l in sm:
# 		print(l)
# 	for k in pvp.verbalizer.label_mapping.keys

	# TODO: calculate loss from actual data depending upon 
	# pattern and iterate over patterns to determine differences between them

	# We can apply this principle to generative functions by using a prompt as a pattern
	# determining a label from the output